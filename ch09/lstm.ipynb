{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "sess = tf.Session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Set RNN Parameters\n",
    "min_word_freq = 5 # Trim the less frequent words off\n",
    "rnn_size = 128 # RNN Model size\n",
    "embedding_size = 100 # Word embedding size\n",
    "epochs = 10 # Number of epochs to cycle through data\n",
    "batch_size = 100 # Train on this many examples at once\n",
    "learning_rate = 0.001 # Learning rate\n",
    "training_seq_len = 50 # how long of a word group to consider \n",
    "embedding_size = rnn_size\n",
    "save_every = 500 # How often to save model checkpoints\n",
    "eval_every = 50 # How often to evaluate the test sentences\n",
    "prime_texts = ['thou art more', 'to be or not to', 'wherefore art thou']\n",
    "\n",
    "# Download/store Shakespeare data\n",
    "data_dir = 'temp'\n",
    "data_file = 'shakespeare.txt'\n",
    "model_path = 'shakespeare_model'\n",
    "full_model_dir = os.path.join(data_dir, model_path)\n",
    "\n",
    "# Declare punctuation to remove, everything except hyphens and apostrophes\n",
    "punctuation = string.punctuation\n",
    "punctuation = ''.join([x for x in punctuation if x not in ['-', \"'\"]])\n",
    "\n",
    "# Make Model Directory\n",
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)\n",
    "\n",
    "# Make data directory\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Loading Shakespeare Data\nNot found, downloading Shakespeare texts from www.gutenberg.org\n",
      "Cleaning Text\n",
      "Done loading/cleaning.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print('Loading Shakespeare Data')\n",
    "# Check if file is downloaded.\n",
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    print('Not found, downloading Shakespeare texts from www.gutenberg.org')\n",
    "    shakespeare_url = 'http://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
    "    # Get Shakespeare text\n",
    "    response = requests.get(shakespeare_url)\n",
    "    shakespeare_file = response.content\n",
    "    # Decode binary into string\n",
    "    s_text = shakespeare_file.decode('utf-8')\n",
    "    # Drop first few descriptive paragraphs.\n",
    "    s_text = s_text[7675:]\n",
    "    # Remove newlines\n",
    "    s_text = s_text.replace('\\r\\n', '')\n",
    "    s_text = s_text.replace('\\n', '')\n",
    "    \n",
    "    # Write to file\n",
    "    with open(os.path.join(data_dir, data_file), 'w') as out_conn:\n",
    "        out_conn.write(s_text)\n",
    "else:\n",
    "    # If file has been saved, load from that file\n",
    "    with open(os.path.join(data_dir, data_file), 'r') as file_conn:\n",
    "        s_text = file_conn.read().replace('\\n', '')\n",
    "\n",
    "# Clean text\n",
    "print('Cleaning Text')\n",
    "s_text = re.sub(r'[{}]'.format(punctuation), ' ', s_text)\n",
    "s_text = re.sub('\\s+', ' ', s_text ).strip().lower()\n",
    "print('Done loading/cleaning.')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Build word vocabulary function\n",
    "def build_vocab(text, min_word_freq):\n",
    "    word_counts = collections.Counter(text.split(' '))\n",
    "    # limit word counts to those more frequent than cutoff\n",
    "    word_counts = {key:val for key, val in word_counts.items() if val>min_word_freq}\n",
    "    # Create vocab --> index mapping\n",
    "    words = word_counts.keys()\n",
    "    vocab_to_ix_dict = {key:(ix+1) for ix, key in enumerate(words)}\n",
    "    # Add unknown key --> 0 index\n",
    "    vocab_to_ix_dict['unknown']=0\n",
    "    # Create index --> vocab mapping\n",
    "    ix_to_vocab_dict = {val:key for key,val in vocab_to_ix_dict.items()}\n",
    "    \n",
    "    return(ix_to_vocab_dict, vocab_to_ix_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Building Shakespeare Vocab\nVocabulary Length = 8009\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Build Shakespeare vocabulary\n",
    "print('Building Shakespeare Vocab')\n",
    "ix2vocab, vocab2ix = build_vocab(s_text, min_word_freq)\n",
    "vocab_size = len(ix2vocab) + 1\n",
    "print('Vocabulary Length = {}'.format(vocab_size))\n",
    "# Sanity Check\n",
    "assert(len(ix2vocab) == len(vocab2ix))\n",
    "\n",
    "# Convert text to word vectors\n",
    "s_text_words = s_text.split(' ')\n",
    "s_text_ix = []\n",
    "for ix, x in enumerate(s_text_words):\n",
    "    try:\n",
    "        s_text_ix.append(vocab2ix[x])\n",
    "    except:\n",
    "        s_text_ix.append(0)\n",
    "s_text_ix = np.array(s_text_ix)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Define LSTM RNN Model\n",
    "class LSTM_Model():\n",
    "    def __init__(self, embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                 training_seq_len, vocab_size, infer_sample=False):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.rnn_size = rnn_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.infer_sample = infer_sample\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if infer_sample:\n",
    "            self.batch_size = 1\n",
    "            self.training_seq_len = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.training_seq_len = training_seq_len\n",
    "        \n",
    "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.rnn_size)\n",
    "        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "        \n",
    "        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        \n",
    "        with tf.variable_scope('lstm_vars'):\n",
    "            # Softmax Output Weights\n",
    "            W = tf.get_variable('W', [self.rnn_size, self.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "            b = tf.get_variable('b', [self.vocab_size], tf.float32, tf.constant_initializer(0.0))\n",
    "        \n",
    "            # Define Embedding\n",
    "            embedding_mat = tf.get_variable('embedding_mat', [self.vocab_size, self.embedding_size],\n",
    "                                            tf.float32, tf.random_normal_initializer())\n",
    "                                            \n",
    "            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)\n",
    "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.training_seq_len, value=embedding_output)\n",
    "            rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "        \n",
    "        # If we are inferring (generating text), we add a 'loop' function\n",
    "        # Define how to get the i+1 th input from the i th output\n",
    "        def inferred_loop(prev, count):\n",
    "            # Apply hidden layer\n",
    "            prev_transformed = tf.matmul(prev, W) + b\n",
    "            # Get the index of the output (also don't run the gradient)\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev_transformed, 1))\n",
    "            # Get embedded vector\n",
    "            output = tf.nn.embedding_lookup(embedding_mat, prev_symbol)\n",
    "            return(output)\n",
    "        \n",
    "        decoder = tf.contrib.legacy_seq2seq.rnn_decoder\n",
    "        outputs, last_state = decoder(rnn_inputs_trimmed,\n",
    "                                      self.initial_state,\n",
    "                                      self.lstm_cell,\n",
    "                                      loop_function=inferred_loop if infer_sample else None)\n",
    "        # Non inferred outputs\n",
    "        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, self.rnn_size])\n",
    "        # Logits and output\n",
    "        self.logit_output = tf.matmul(output, W) + b\n",
    "        self.model_output = tf.nn.softmax(self.logit_output)\n",
    "        \n",
    "        loss_fun = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n",
    "        loss = loss_fun([self.logit_output],[tf.reshape(self.y_output, [-1])],\n",
    "                [tf.ones([self.batch_size * self.training_seq_len])],\n",
    "                self.vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / (self.batch_size * self.training_seq_len)\n",
    "        self.final_state = last_state\n",
    "        gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), 4.5)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
    "        \n",
    "    def sample(self, sess, words=ix2vocab, vocab=vocab2ix, num=10, prime_text='thou art'):\n",
    "        state = sess.run(self.lstm_cell.zero_state(1, tf.float32))\n",
    "        word_list = prime_text.split()\n",
    "        for word in word_list[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed_dict=feed_dict)\n",
    "\n",
    "        out_sentence = prime_text\n",
    "        word = word_list[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [model_output, state] = sess.run([self.model_output, self.final_state], feed_dict=feed_dict)\n",
    "            sample = np.argmax(model_output[0])\n",
    "            if sample == 0:\n",
    "                break\n",
    "            word = words[sample]\n",
    "            out_sentence = out_sentence + ' ' + word\n",
    "        return(out_sentence)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\nIf you depend on functionality not listed there, please file an issue.\n\nWARNING:tensorflow:From <ipython-input-10-37dc152199aa>:18: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Define LSTM Model\n",
    "lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                        training_seq_len, vocab_size)\n",
    "\n",
    "# Tell TensorFlow we are reusing the scope for the testing\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "    test_lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                                 training_seq_len, vocab_size, infer_sample=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Create model saver\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Create batches for each epoch\n",
    "num_batches = int(len(s_text_ix)/(batch_size * training_seq_len)) + 1\n",
    "# Split up text indices into subarrays, of equal size\n",
    "batches = np.array_split(s_text_ix, num_batches)\n",
    "# Reshape each split into [batch_size, training_seq_len]\n",
    "batches = [np.resize(x, [batch_size, training_seq_len]) for x in batches]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Initialize all variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Starting Epoch #1 of 10.\n",
      "Iteration: 10, Epoch: 1, Batch: 10 out of 182, Loss: 9.95\n",
      "Iteration: 20, Epoch: 1, Batch: 20 out of 182, Loss: 9.17\n",
      "Iteration: 30, Epoch: 1, Batch: 30 out of 182, Loss: 8.54\n",
      "Iteration: 40, Epoch: 1, Batch: 40 out of 182, Loss: 8.19\n",
      "Iteration: 50, Epoch: 1, Batch: 50 out of 182, Loss: 8.05\nthou art more defence in the\nto be or not to\n",
      "wherefore art thou art mine gon mine aguecheek stale trumpets aguecheek taint taint\n",
      "Iteration: 60, Epoch: 1, Batch: 60 out of 182, Loss: 7.77\n",
      "Iteration: 70, Epoch: 1, Batch: 70 out of 182, Loss: 7.71\n",
      "Iteration: 80, Epoch: 1, Batch: 80 out of 182, Loss: 7.42\n",
      "Iteration: 90, Epoch: 1, Batch: 90 out of 182, Loss: 7.34\n",
      "Iteration: 100, Epoch: 1, Batch: 100 out of 182, Loss: 7.23\nthou art more than in\nto be or not to the\n",
      "wherefore art thou art a\n",
      "Iteration: 110, Epoch: 1, Batch: 110 out of 182, Loss: 7.01\n",
      "Iteration: 120, Epoch: 1, Batch: 120 out of 182, Loss: 6.93\n",
      "Iteration: 130, Epoch: 1, Batch: 130 out of 182, Loss: 6.65\n",
      "Iteration: 140, Epoch: 1, Batch: 140 out of 182, Loss: 6.88\n",
      "Iteration: 150, Epoch: 1, Batch: 150 out of 182, Loss: 6.74\nthou art more than thou\nto be or not to the\n",
      "wherefore art thou hast thou art thou art thou\n",
      "Iteration: 160, Epoch: 1, Batch: 160 out of 182, Loss: 6.78\n",
      "Iteration: 170, Epoch: 1, Batch: 170 out of 182, Loss: 6.61\n",
      "Iteration: 180, Epoch: 1, Batch: 180 out of 182, Loss: 6.65\n",
      "Starting Epoch #2 of 10.\n",
      "Iteration: 190, Epoch: 2, Batch: 9 out of 182, Loss: 6.62\n",
      "Iteration: 200, Epoch: 2, Batch: 19 out of 182, Loss: 6.50\nthou art more than thou art thou art thou\nto be or not to be\n",
      "wherefore art thou art peter not in the\n",
      "Iteration: 210, Epoch: 2, Batch: 29 out of 182, Loss: 6.54\n",
      "Iteration: 220, Epoch: 2, Batch: 39 out of 182, Loss: 6.23\n",
      "Iteration: 230, Epoch: 2, Batch: 49 out of 182, Loss: 6.54\n",
      "Iteration: 240, Epoch: 2, Batch: 59 out of 182, Loss: 6.42\n",
      "Iteration: 250, Epoch: 2, Batch: 69 out of 182, Loss: 6.46\nthou art more than in a\nto be or not to be\n",
      "wherefore art thou art peter not in the\n",
      "Iteration: 260, Epoch: 2, Batch: 79 out of 182, Loss: 6.33\n",
      "Iteration: 270, Epoch: 2, Batch: 89 out of 182, Loss: 6.18\n",
      "Iteration: 280, Epoch: 2, Batch: 99 out of 182, Loss: 6.44\n",
      "Iteration: 290, Epoch: 2, Batch: 109 out of 182, Loss: 6.35\n",
      "Iteration: 300, Epoch: 2, Batch: 119 out of 182, Loss: 6.36\nthou art more than in the\n",
      "to be or not to be\nwherefore art thou rather art peter in the\n",
      "Iteration: 310, Epoch: 2, Batch: 129 out of 182, Loss: 6.24\n",
      "Iteration: 320, Epoch: 2, Batch: 139 out of 182, Loss: 6.36\n",
      "Iteration: 330, Epoch: 2, Batch: 149 out of 182, Loss: 6.31\n",
      "Iteration: 340, Epoch: 2, Batch: 159 out of 182, Loss: 6.08\n",
      "Iteration: 350, Epoch: 2, Batch: 169 out of 182, Loss: 6.19\nthou art more than than to be\nto be or not to be\n",
      "wherefore art thou rather smock wilt thou art thou art thou art thou\n",
      "Iteration: 360, Epoch: 2, Batch: 179 out of 182, Loss: 6.27\n",
      "Starting Epoch #3 of 10.\n",
      "Iteration: 370, Epoch: 3, Batch: 8 out of 182, Loss: 6.52\n",
      "Iteration: 380, Epoch: 3, Batch: 18 out of 182, Loss: 6.55\n",
      "Iteration: 390, Epoch: 3, Batch: 28 out of 182, Loss: 6.54\n",
      "Iteration: 400, Epoch: 3, Batch: 38 out of 182, Loss: 6.65\nthou art more than than thou art thou art thou art thou art\nto be or not to be\n",
      "wherefore art thou rather smock wilt thou art thou art thou art thou\n",
      "Iteration: 410, Epoch: 3, Batch: 48 out of 182, Loss: 6.31\n",
      "Iteration: 420, Epoch: 3, Batch: 58 out of 182, Loss: 6.28\n",
      "Iteration: 430, Epoch: 3, Batch: 68 out of 182, Loss: 6.40\n",
      "Iteration: 440, Epoch: 3, Batch: 78 out of 182, Loss: 6.30\n",
      "Iteration: 450, Epoch: 3, Batch: 88 out of 182, Loss: 6.38\nthou art more than than to be\nto be or not to be\n",
      "wherefore art thou rather middle shameless middle posterity teaches heads blanch which\n",
      "Iteration: 460, Epoch: 3, Batch: 98 out of 182, Loss: 6.09\n",
      "Iteration: 470, Epoch: 3, Batch: 108 out of 182, Loss: 6.00\n",
      "Iteration: 480, Epoch: 3, Batch: 118 out of 182, Loss: 6.26\n",
      "Iteration: 490, Epoch: 3, Batch: 128 out of 182, Loss: 6.19\n",
      "Iteration: 500, Epoch: 3, Batch: 138 out of 182, Loss: 5.98\n",
      "Model Saved To: temp/shakespeare_model/model\nthou art more than than to be\nto be or not to be\n",
      "wherefore art thou rather smock wilt thou art thou art thou art thou\n",
      "Iteration: 510, Epoch: 3, Batch: 148 out of 182, Loss: 6.20\n",
      "Iteration: 520, Epoch: 3, Batch: 158 out of 182, Loss: 6.28\n",
      "Iteration: 530, Epoch: 3, Batch: 168 out of 182, Loss: 6.03\n",
      "Iteration: 540, Epoch: 3, Batch: 178 out of 182, Loss: 6.03\n",
      "Starting Epoch #4 of 10.\n",
      "Iteration: 550, Epoch: 4, Batch: 7 out of 182, Loss: 6.19\nthou art more than than to be\nto be or not to be\n",
      "wherefore art thou rather v thee for thy\n",
      "Iteration: 560, Epoch: 4, Batch: 17 out of 182, Loss: 6.28\n",
      "Iteration: 570, Epoch: 4, Batch: 27 out of 182, Loss: 6.25\n",
      "Iteration: 580, Epoch: 4, Batch: 37 out of 182, Loss: 6.07\n",
      "Iteration: 590, Epoch: 4, Batch: 47 out of 182, Loss: 6.09\n",
      "Iteration: 600, Epoch: 4, Batch: 57 out of 182, Loss: 5.91\nthou art more than than to be\nto be or not to be\n",
      "wherefore art thou rather middle come come in the\n",
      "Iteration: 610, Epoch: 4, Batch: 67 out of 182, Loss: 6.30\n",
      "Iteration: 620, Epoch: 4, Batch: 77 out of 182, Loss: 6.24\n",
      "Iteration: 630, Epoch: 4, Batch: 87 out of 182, Loss: 6.13\n",
      "Iteration: 640, Epoch: 4, Batch: 97 out of 182, Loss: 6.28\n",
      "Iteration: 650, Epoch: 4, Batch: 107 out of 182, Loss: 6.31\nthou art more than than to be\nto be or not to the\n",
      "wherefore art thou rather middle come come to be\n",
      "Iteration: 660, Epoch: 4, Batch: 117 out of 182, Loss: 6.18\n",
      "Iteration: 670, Epoch: 4, Batch: 127 out of 182, Loss: 6.06\n",
      "Iteration: 680, Epoch: 4, Batch: 137 out of 182, Loss: 6.17\n",
      "Iteration: 690, Epoch: 4, Batch: 147 out of 182, Loss: 5.86\n",
      "Iteration: 700, Epoch: 4, Batch: 157 out of 182, Loss: 5.93\nthou art more than than a\nto be or not to be\n",
      "wherefore art thou dost dost think think think so not a\n",
      "Iteration: 710, Epoch: 4, Batch: 167 out of 182, Loss: 6.03\n",
      "Iteration: 720, Epoch: 4, Batch: 177 out of 182, Loss: 6.05\n",
      "Starting Epoch #5 of 10.\n",
      "Iteration: 730, Epoch: 5, Batch: 6 out of 182, Loss: 5.96\n",
      "Iteration: 740, Epoch: 5, Batch: 16 out of 182, Loss: 5.93\n",
      "Iteration: 750, Epoch: 5, Batch: 26 out of 182, Loss: 6.22\nthou art more than than a\n",
      "to be or not to be\nwherefore art thou rather didst thou art a\n",
      "Iteration: 760, Epoch: 5, Batch: 36 out of 182, Loss: 6.04\n",
      "Iteration: 770, Epoch: 5, Batch: 46 out of 182, Loss: 5.73\n",
      "Iteration: 780, Epoch: 5, Batch: 56 out of 182, Loss: 6.22\n",
      "Iteration: 790, Epoch: 5, Batch: 66 out of 182, Loss: 5.90\n",
      "Iteration: 800, Epoch: 5, Batch: 76 out of 182, Loss: 5.99\nthou art more than than to be\nto be or not to the\n",
      "wherefore art thou rather didst little than you are\n",
      "Iteration: 810, Epoch: 5, Batch: 86 out of 182, Loss: 6.10\n",
      "Iteration: 820, Epoch: 5, Batch: 96 out of 182, Loss: 5.66\n",
      "Iteration: 830, Epoch: 5, Batch: 106 out of 182, Loss: 6.17\n",
      "Iteration: 840, Epoch: 5, Batch: 116 out of 182, Loss: 5.83\n",
      "Iteration: 850, Epoch: 5, Batch: 126 out of 182, Loss: 6.07\nthou art more than than to be\nto be or not to the\n",
      "wherefore art thou rather didst didst thou art not a man that i\n",
      "Iteration: 860, Epoch: 5, Batch: 136 out of 182, Loss: 5.93\n",
      "Iteration: 870, Epoch: 5, Batch: 146 out of 182, Loss: 5.70\n",
      "Iteration: 880, Epoch: 5, Batch: 156 out of 182, Loss: 5.91\n",
      "Iteration: 890, Epoch: 5, Batch: 166 out of 182, Loss: 6.14\n",
      "Iteration: 900, Epoch: 5, Batch: 176 out of 182, Loss: 5.84\nthou art more than a\nto be or not to the\n",
      "wherefore art thou rather didst thou art a\n",
      "Starting Epoch #6 of 10.\n",
      "Iteration: 910, Epoch: 6, Batch: 5 out of 182, Loss: 5.62\n",
      "Iteration: 920, Epoch: 6, Batch: 15 out of 182, Loss: 6.01\n",
      "Iteration: 930, Epoch: 6, Batch: 25 out of 182, Loss: 5.94\n",
      "Iteration: 940, Epoch: 6, Batch: 35 out of 182, Loss: 5.95\n",
      "Iteration: 950, Epoch: 6, Batch: 45 out of 182, Loss: 5.96\nthou art more than to be\nto be or not to the\n",
      "wherefore art thou dost dost think think not think it is not to\n",
      "Iteration: 960, Epoch: 6, Batch: 55 out of 182, Loss: 6.09\n",
      "Iteration: 970, Epoch: 6, Batch: 65 out of 182, Loss: 6.04\n",
      "Iteration: 980, Epoch: 6, Batch: 75 out of 182, Loss: 5.92\n",
      "Iteration: 990, Epoch: 6, Batch: 85 out of 182, Loss: 5.79\n",
      "Iteration: 1000, Epoch: 6, Batch: 95 out of 182, Loss: 5.92\nModel Saved To: temp/shakespeare_model/model\n",
      "thou art more than a\nto be or not to be\n",
      "wherefore art thou rather didst didst thou art a\n",
      "Iteration: 1010, Epoch: 6, Batch: 105 out of 182, Loss: 6.00\n",
      "Iteration: 1020, Epoch: 6, Batch: 115 out of 182, Loss: 5.84\n",
      "Iteration: 1030, Epoch: 6, Batch: 125 out of 182, Loss: 5.87\n",
      "Iteration: 1040, Epoch: 6, Batch: 135 out of 182, Loss: 6.20\n",
      "Iteration: 1050, Epoch: 6, Batch: 145 out of 182, Loss: 6.04\nthou art more than a\n",
      "to be or not to the\nwherefore art thou shalt be not to be\n",
      "Iteration: 1060, Epoch: 6, Batch: 155 out of 182, Loss: 6.19\n",
      "Iteration: 1070, Epoch: 6, Batch: 165 out of 182, Loss: 5.80\n",
      "Iteration: 1080, Epoch: 6, Batch: 175 out of 182, Loss: 6.09\n",
      "Starting Epoch #7 of 10.\n",
      "Iteration: 1090, Epoch: 7, Batch: 4 out of 182, Loss: 5.85\n",
      "Iteration: 1100, Epoch: 7, Batch: 14 out of 182, Loss: 5.93\nthou art more than a\n",
      "to be or not to the\nwherefore art thou shalt have a\n",
      "Iteration: 1110, Epoch: 7, Batch: 24 out of 182, Loss: 5.93\n",
      "Iteration: 1120, Epoch: 7, Batch: 34 out of 182, Loss: 5.94\n",
      "Iteration: 1130, Epoch: 7, Batch: 44 out of 182, Loss: 5.94\n",
      "Iteration: 1140, Epoch: 7, Batch: 54 out of 182, Loss: 5.80\n",
      "Iteration: 1150, Epoch: 7, Batch: 64 out of 182, Loss: 5.89\nthou art more than a\n",
      "to be or not to the\nwherefore art thou shalt have a\n",
      "Iteration: 1160, Epoch: 7, Batch: 74 out of 182, Loss: 5.92\n",
      "Iteration: 1170, Epoch: 7, Batch: 84 out of 182, Loss: 5.81\n",
      "Iteration: 1180, Epoch: 7, Batch: 94 out of 182, Loss: 5.98\n",
      "Iteration: 1190, Epoch: 7, Batch: 104 out of 182, Loss: 5.88\n",
      "Iteration: 1200, Epoch: 7, Batch: 114 out of 182, Loss: 5.75\nthou art more than a\n",
      "to be or not to the\nwherefore art thou shalt be not to be\n",
      "Iteration: 1210, Epoch: 7, Batch: 124 out of 182, Loss: 6.03\n",
      "Iteration: 1220, Epoch: 7, Batch: 134 out of 182, Loss: 5.86\n",
      "Iteration: 1230, Epoch: 7, Batch: 144 out of 182, Loss: 5.88\n",
      "Iteration: 1240, Epoch: 7, Batch: 154 out of 182, Loss: 5.63\n",
      "Iteration: 1250, Epoch: 7, Batch: 164 out of 182, Loss: 5.89\nthou art more than a\n",
      "to be or not to be\nwherefore art thou shalt drink that i am not to the\n",
      "Iteration: 1260, Epoch: 7, Batch: 174 out of 182, Loss: 5.77\n",
      "Starting Epoch #8 of 10.\n",
      "Iteration: 1270, Epoch: 8, Batch: 3 out of 182, Loss: 6.12\n",
      "Iteration: 1280, Epoch: 8, Batch: 13 out of 182, Loss: 5.81\n",
      "Iteration: 1290, Epoch: 8, Batch: 23 out of 182, Loss: 5.78\n",
      "Iteration: 1300, Epoch: 8, Batch: 33 out of 182, Loss: 5.90\nthou art more than a\n",
      "to be or not to be\nwherefore art thou know'st rather didst thou art a\n",
      "Iteration: 1310, Epoch: 8, Batch: 43 out of 182, Loss: 5.97\n",
      "Iteration: 1320, Epoch: 8, Batch: 53 out of 182, Loss: 5.85\n",
      "Iteration: 1330, Epoch: 8, Batch: 63 out of 182, Loss: 5.98\n",
      "Iteration: 1340, Epoch: 8, Batch: 73 out of 182, Loss: 6.00\n",
      "Iteration: 1350, Epoch: 8, Batch: 83 out of 182, Loss: 5.75\nthou art more than a\n",
      "to be or not to the\nwherefore art thou shalt blame thee for thy\n",
      "Iteration: 1360, Epoch: 8, Batch: 93 out of 182, Loss: 5.76\n",
      "Iteration: 1370, Epoch: 8, Batch: 103 out of 182, Loss: 5.62\n",
      "Iteration: 1380, Epoch: 8, Batch: 113 out of 182, Loss: 5.89\n",
      "Iteration: 1390, Epoch: 8, Batch: 123 out of 182, Loss: 5.74\n",
      "Iteration: 1400, Epoch: 8, Batch: 133 out of 182, Loss: 5.85\nthou art more than a\n",
      "to be or not to the\nwherefore art thou shalt have a\n",
      "Iteration: 1410, Epoch: 8, Batch: 143 out of 182, Loss: 5.86\n",
      "Iteration: 1420, Epoch: 8, Batch: 153 out of 182, Loss: 5.89\n",
      "Iteration: 1430, Epoch: 8, Batch: 163 out of 182, Loss: 5.96\n",
      "Iteration: 1440, Epoch: 8, Batch: 173 out of 182, Loss: 6.11\n",
      "Starting Epoch #9 of 10.\n",
      "Iteration: 1450, Epoch: 9, Batch: 2 out of 182, Loss: 5.70\nthou art more than a\n",
      "to be or not to be\nwherefore art thou shalt have a\n",
      "Iteration: 1460, Epoch: 9, Batch: 12 out of 182, Loss: 6.01\n",
      "Iteration: 1470, Epoch: 9, Batch: 22 out of 182, Loss: 5.90\n",
      "Iteration: 1480, Epoch: 9, Batch: 32 out of 182, Loss: 5.68\n",
      "Iteration: 1490, Epoch: 9, Batch: 42 out of 182, Loss: 5.86\n",
      "Iteration: 1500, Epoch: 9, Batch: 52 out of 182, Loss: 5.57\n",
      "Model Saved To: temp/shakespeare_model/model\nthou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou shalt have a\n",
      "Iteration: 1510, Epoch: 9, Batch: 62 out of 182, Loss: 5.75\n",
      "Iteration: 1520, Epoch: 9, Batch: 72 out of 182, Loss: 5.79\n",
      "Iteration: 1530, Epoch: 9, Batch: 82 out of 182, Loss: 5.52\n",
      "Iteration: 1540, Epoch: 9, Batch: 92 out of 182, Loss: 5.83\n",
      "Iteration: 1550, Epoch: 9, Batch: 102 out of 182, Loss: 5.87\nthou art more than a\n",
      "to be or not to be\nwherefore art thou speak'st sick which thou hast not so\n",
      "Iteration: 1560, Epoch: 9, Batch: 112 out of 182, Loss: 5.62\n",
      "Iteration: 1570, Epoch: 9, Batch: 122 out of 182, Loss: 6.02\n",
      "Iteration: 1580, Epoch: 9, Batch: 132 out of 182, Loss: 5.76\n",
      "Iteration: 1590, Epoch: 9, Batch: 142 out of 182, Loss: 5.82\n",
      "Iteration: 1600, Epoch: 9, Batch: 152 out of 182, Loss: 5.43\nthou art more than a\n",
      "to be or not to be\nwherefore art thou shalt blame thee for thy\n",
      "Iteration: 1610, Epoch: 9, Batch: 162 out of 182, Loss: 5.91\n",
      "Iteration: 1620, Epoch: 9, Batch: 172 out of 182, Loss: 5.88\n",
      "Starting Epoch #10 of 10.\n",
      "Iteration: 1630, Epoch: 10, Batch: 1 out of 182, Loss: 5.80\n",
      "Iteration: 1640, Epoch: 10, Batch: 11 out of 182, Loss: 5.89\n",
      "Iteration: 1650, Epoch: 10, Batch: 21 out of 182, Loss: 5.65\nthou art more than a\n",
      "to be or not to the\nwherefore art thou shalt have a\n",
      "Iteration: 1660, Epoch: 10, Batch: 31 out of 182, Loss: 5.97\n",
      "Iteration: 1670, Epoch: 10, Batch: 41 out of 182, Loss: 5.77\n",
      "Iteration: 1680, Epoch: 10, Batch: 51 out of 182, Loss: 5.84\n",
      "Iteration: 1690, Epoch: 10, Batch: 61 out of 182, Loss: 5.77\n",
      "Iteration: 1700, Epoch: 10, Batch: 71 out of 182, Loss: 5.47\nthou art more than a\n",
      "to be or not to be\nwherefore art thou shalt have a\n",
      "Iteration: 1710, Epoch: 10, Batch: 81 out of 182, Loss: 5.62\n",
      "Iteration: 1720, Epoch: 10, Batch: 91 out of 182, Loss: 5.49\n",
      "Iteration: 1730, Epoch: 10, Batch: 101 out of 182, Loss: 5.74\n",
      "Iteration: 1740, Epoch: 10, Batch: 111 out of 182, Loss: 5.63\n",
      "Iteration: 1750, Epoch: 10, Batch: 121 out of 182, Loss: 5.76\nthou art more than a\n",
      "to be or not to the\nwherefore art thou shalt have a\n",
      "Iteration: 1760, Epoch: 10, Batch: 131 out of 182, Loss: 5.45\n",
      "Iteration: 1770, Epoch: 10, Batch: 141 out of 182, Loss: 5.93\n",
      "Iteration: 1780, Epoch: 10, Batch: 151 out of 182, Loss: 6.04\n",
      "Iteration: 1790, Epoch: 10, Batch: 161 out of 182, Loss: 5.84\n",
      "Iteration: 1800, Epoch: 10, Batch: 171 out of 182, Loss: 5.73\nthou art more than a\n",
      "to be or not to be\nwherefore art thou shalt have a\n",
      "Iteration: 1810, Epoch: 10, Batch: 181 out of 182, Loss: 5.65\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Train model\n",
    "train_loss = []\n",
    "iteration_count = 1\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle word indices\n",
    "    random.shuffle(batches)\n",
    "    # Create targets from shuffled batches\n",
    "    targets = [np.roll(x, -1, axis=1) for x in batches]\n",
    "    # Run a through one epoch\n",
    "    print('Starting Epoch #{} of {}.'.format(epoch+1, epochs))\n",
    "    # Reset initial LSTM state every epoch\n",
    "    state = sess.run(lstm_model.initial_state)\n",
    "    for ix, batch in enumerate(batches):\n",
    "        training_dict = {lstm_model.x_data: batch, lstm_model.y_output: targets[ix]}\n",
    "        c, h = lstm_model.initial_state\n",
    "        training_dict[c] = state.c\n",
    "        training_dict[h] = state.h\n",
    "        \n",
    "        temp_loss, state, _ = sess.run([lstm_model.cost, lstm_model.final_state, lstm_model.train_op],\n",
    "                                       feed_dict=training_dict)\n",
    "        train_loss.append(temp_loss)\n",
    "        \n",
    "        # Print status every 10 gens\n",
    "        if iteration_count % 10 == 0:\n",
    "            summary_nums = (iteration_count, epoch+1, ix+1, num_batches+1, temp_loss)\n",
    "            print('Iteration: {}, Epoch: {}, Batch: {} out of {}, Loss: {:.2f}'.format(*summary_nums))\n",
    "        \n",
    "        # Save the model and the vocab\n",
    "        if iteration_count % save_every == 0:\n",
    "            # Save model\n",
    "            model_file_name = os.path.join(full_model_dir, 'model')\n",
    "            saver.save(sess, model_file_name, global_step = iteration_count)\n",
    "            print('Model Saved To: {}'.format(model_file_name))\n",
    "            # Save vocabulary\n",
    "            dictionary_file = os.path.join(full_model_dir, 'vocab.pkl')\n",
    "            with open(dictionary_file, 'wb') as dict_file_conn:\n",
    "                pickle.dump([vocab2ix, ix2vocab], dict_file_conn)\n",
    "        \n",
    "        if iteration_count % eval_every == 0:\n",
    "            for sample in prime_texts:\n",
    "                print(test_lstm_model.sample(sess, ix2vocab, vocab2ix, num=10, prime_text=sample))\n",
    "                \n",
    "        iteration_count += 1\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss over time\n",
    "plt.plot(train_loss, 'k-')\n",
    "plt.title('Sequence to Sequence Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}